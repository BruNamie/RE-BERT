{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting and saving requirements"
      ],
      "metadata": {
        "id": "rTj660p4N8lA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a notebook that shows how to extract requirements from a test dataset using a specific model. We will be extracting them eight times, once for each model trained for the Cross-Validation method. "
      ],
      "metadata": {
        "id": "yuB-vyumOH80"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "UjVjtDUrEX1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connecting to google drive to save the extracted requirements there. "
      ],
      "metadata": {
        "id": "obl0GPFbPcLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import shutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOFvrUvGHN6j",
        "outputId": "f53d7b85-1d52-4beb-868a-894fb4d27c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning a RE-BERT repository."
      ],
      "metadata": {
        "id": "VHn7z1A9Eqy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/BruNamie/RE-BERT\n",
        "!mv RE-BERT/* ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkIAaXvvEmeD",
        "outputId": "cec79a52-1c19-43af-ee25-71af21eab3de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RE-BERT'...\n",
            "remote: Enumerating objects: 174, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 174 (delta 54), reused 59 (delta 32), pack-reused 86\u001b[K\n",
            "Receiving objects: 100% (174/174), 1.89 MiB | 37.92 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2kU778bieDi"
      },
      "source": [
        "Installing dependencies."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO88tArXEoUd",
        "outputId": "033985ce-0b89-4449-f83e-9374378b3405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.12.1+cu113)\n",
            "Collecting transformers==2.3.0\n",
            "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\n",
            "\u001b[K     |████████████████████████████████| 447 kB 20.5 MB/s \n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 59.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (4.64.1)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.25.2-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.3.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->-r requirements.txt (line 2)) (4.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 4)) (1.0.2)\n",
            "Collecting botocore<1.29.0,>=1.28.2\n",
            "  Downloading botocore-1.28.2-py3-none-any.whl (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 54.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.2->boto3->transformers==2.3.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.2->boto3->transformers==2.3.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 74.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.3.0->-r requirements.txt (line 3)) (2022.9.24)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.3.0->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (3.1.0)\n",
            "Building wheels for collected packages: sklearn, sacremoses\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1310 sha256=803f6343f69458095117055b4f5bdca6273b884f1413f6af4e076fa5354bb69c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=c87f1cbb4752fb4ad1a8c36609849f4dbb7160d7d66b7d8f21785f5ed03e75d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sklearn sacremoses\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, transformers, sklearn\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed boto3-1.25.2 botocore-1.28.2 jmespath-1.0.1 s3transfer-0.6.0 sacremoses-0.0.53 sentencepiece-0.1.97 sklearn-0.0 transformers-2.3.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv0yuEUJyMyQ"
      },
      "source": [
        "Downloading pre-trained RE-BERT models. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwIZ6x1ED3BC",
        "outputId": "81877018-ebcd-4209-8573-b71687e2ea16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TWNGsbbAmX04jwJn5o1-wxYsCbGZaUF9\n",
            "To: /content/RE_BERT_CV0_iob_epoch_1.model\n",
            "100% 455M/455M [00:03<00:00, 136MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Jhhx8Q7tCCkTTW6OoM55Qi3wxO77hV34\n",
            "To: /content/RE_BERT_CV1_iob_epoch_1.model\n",
            "100% 455M/455M [00:04<00:00, 104MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1T6vQxPVtYb1-8Txh8EA0DLDESJQ-AR7v\n",
            "To: /content/RE_BERT_CV2_iob_epoch_1.model\n",
            "100% 455M/455M [00:03<00:00, 119MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ExynBLdpgwApjq2ZzbiEPGpVX-HhHDMH\n",
            "To: /content/RE_BERT_CV3_iob_epoch_1.model\n",
            "100% 455M/455M [00:03<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GjVLHaniahnByz2L7FSd56wumiLzv2Hl\n",
            "To: /content/RE_BERT_CV4_iob_epoch_1.model\n",
            "100% 455M/455M [00:06<00:00, 68.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xi06phUSfELiDMQvjqduv9NpHu3ybZDo\n",
            "To: /content/RE_BERT_CV5_iob_epoch_1.model\n",
            "100% 455M/455M [00:07<00:00, 57.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1otYsNilO5Z8z94tlc6NrGdZfQvTc1HuK\n",
            "To: /content/RE_BERT_CV6_iob_epoch_1.model\n",
            "100% 455M/455M [00:02<00:00, 169MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11U6QSpASqfMoAcXtKerfaXrasSixXHHu\n",
            "To: /content/RE_BERT_CV7_iob_epoch_1.model\n",
            "100% 455M/455M [00:02<00:00, 155MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1TWNGsbbAmX04jwJn5o1-wxYsCbGZaUF9\n",
        "!gdown https://drive.google.com/uc?id=1Jhhx8Q7tCCkTTW6OoM55Qi3wxO77hV34\n",
        "!gdown https://drive.google.com/uc?id=1T6vQxPVtYb1-8Txh8EA0DLDESJQ-AR7v\n",
        "!gdown https://drive.google.com/uc?id=1ExynBLdpgwApjq2ZzbiEPGpVX-HhHDMH\n",
        "!gdown https://drive.google.com/uc?id=1GjVLHaniahnByz2L7FSd56wumiLzv2Hl\n",
        "!gdown https://drive.google.com/uc?id=1xi06phUSfELiDMQvjqduv9NpHu3ybZDo\n",
        "!gdown https://drive.google.com/uc?id=1otYsNilO5Z8z94tlc6NrGdZfQvTc1HuK\n",
        "!gdown https://drive.google.com/uc?id=11U6QSpASqfMoAcXtKerfaXrasSixXHHu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_models = ['RE_BERT_CV0_iob_epoch_1.model', 'RE_BERT_CV1_iob_epoch_1.model', 'RE_BERT_CV2_iob_epoch_1.model', 'RE_BERT_CV3_iob_epoch_1.model',\n",
        "                      'RE_BERT_CV4_iob_epoch_1.model', 'RE_BERT_CV5_iob_epoch_1.model', 'RE_BERT_CV6_iob_epoch_1.model', 'RE_BERT_CV7_iob_epoch_1.model']"
      ],
      "metadata": {
        "id": "CBioxZG7GJ1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting software requirements"
      ],
      "metadata": {
        "id": "UlVjKr21FLam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that the codes presented below are repeated for each iteration. The process to extract requirements was carried out in this way due to limitations of Google Colab, the platform used for this. "
      ],
      "metadata": {
        "id": "6J_J8h8MQrwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #1"
      ],
      "metadata": {
        "id": "3mDUQuY3FC-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '0'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "9HdC7Z5iFEsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3VSP-89FILK",
        "outputId": "e7038ba2-dea4-4124-a8c6-9e70205abec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "loading model RE_BERT ...\n",
            "Test Size: 367\n",
            "Extract software requirements candidates:   0% 0/367 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 367/367 [02:53<00:00,  2.11it/s]\n",
            "Extracted software requirements --> CV_0_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZSihT-FrGkGi",
        "outputId": "44677180-1010-413e-8407-d5b6aa0a1580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_0_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #2"
      ],
      "metadata": {
        "id": "kArHEyzaHUA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '1'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "F4IHjDaaHVZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BINAevk6HbXs",
        "outputId": "94c0c42b-efdc-4fd3-8015-424407508c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 327\n",
            "Extract software requirements candidates:   0% 0/327 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 327/327 [02:04<00:00,  2.62it/s]\n",
            "Extracted software requirements --> CV_1_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_ZMLhZMbHc9J",
        "outputId": "87a8a1b1-7bd5-419c-a8f4-72fe5d9e3641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_1_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #3"
      ],
      "metadata": {
        "id": "fSQmL3fMHefk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '2'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "HZQAo_h0IMn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "id": "Ub0MlHiiIOq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3747bad4-2662-493e-e1a9-3d46f54d0d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 341\n",
            "Extract software requirements candidates:   0% 0/341 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 341/341 [02:23<00:00,  2.37it/s]\n",
            "Extracted software requirements --> CV_2_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "id": "ByqdG1gzIP-T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7058f3c5-9d94-4cc7-83a9-0b50d663b6aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_2_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #4"
      ],
      "metadata": {
        "id": "LNxqEtrbHlTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '3'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "qJJWGPDXISlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "id": "HlOmBYA0IUl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "332eee3f-839e-4983-ab05-11edf28aa852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 154\n",
            "Extract software requirements candidates:   0% 0/154 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 154/154 [00:40<00:00,  3.82it/s]\n",
            "Extracted software requirements --> CV_3_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "id": "84790U8ZIWGs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0b6d7ea7-d12d-4b20-8173-5f4d8c1bd339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_3_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #5"
      ],
      "metadata": {
        "id": "D6VBiL7sH5d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '4'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "ZTEr5menI_6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWPPBrG4JBW_",
        "outputId": "3d51c003-8963-42a9-8f1f-9ebb6e5a02f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 227\n",
            "Extract software requirements candidates:   0% 0/227 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 227/227 [01:16<00:00,  2.97it/s]\n",
            "Extracted software requirements --> CV_4_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "788JmQW7JCGN",
        "outputId": "4ff41437-fd72-45ea-ff4b-7661dc0c1ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_4_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #6"
      ],
      "metadata": {
        "id": "LSotpsU5JCtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '5'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "Yn-V_6XwJGBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chLPgeH8JFPm",
        "outputId": "8d4d75d0-63de-47a2-816b-29a6d5e70d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 183\n",
            "Extract software requirements candidates:   0% 0/183 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 183/183 [00:49<00:00,  3.69it/s]\n",
            "Extracted software requirements --> CV_5_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8ahogpW0JD3f",
        "outputId": "cce546ed-3c61-4a8e-8bfc-3c8e555a2119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_5_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #7"
      ],
      "metadata": {
        "id": "C4RBpa-zJI9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '6'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "sqVPcCdBJJ_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-7Y_Pp9JMBc",
        "outputId": "5fda6f53-0521-4db3-9933-20d9704a7688"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 169\n",
            "Extract software requirements candidates:   0% 0/169 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 169/169 [00:49<00:00,  3.41it/s]\n",
            "Extracted software requirements --> CV_6_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njrWxQ2XJMq5",
        "outputId": "bf66c300-639b-4fd2-92d0-4b2e2bc5340d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_6_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choosing model #8"
      ],
      "metadata": {
        "id": "PqGkRvLeJNix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV_number = '7'\n",
        "model_chosen = pre_trained_models[int(CV_number)]"
      ],
      "metadata": {
        "id": "3sqiMx72N5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python extract.py --test_file test_data_CV_{CV_number}.txt --classifier_model_file RE_BERT_CV{CV_number}_iob_epoch_1.model --output_file CV_{CV_number}_extracted_reqs.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfJprYkIJZFH",
        "outputId": "bd84ffa3-5282-492d-d8af-ce3e4ed3bc5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "loading model RE_BERT ...\n",
            "Test Size: 294\n",
            "Extract software requirements candidates:   0% 0/294 [00:00<?, ?it/s]extract.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  t_inputs = [torch.tensor([data[col]], device=self.opt.device) for col in self.opt.inputs_cols]\n",
            "Extract software requirements candidates: 100% 294/294 [01:53<00:00,  2.60it/s]\n",
            "Extracted software requirements --> CV_7_extracted_reqs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.copy(\"CV_\"+CV_number+\"_extracted_reqs.txt\",\"/content/drive/MyDrive/Colab Notebooks/Requirements_extracted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxhCtY1SJaDv",
        "outputId": "01a60708-21a6-4f0e-ccb6-e25a938acb0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks/Requirements_extracted/CV_7_extracted_reqs.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}